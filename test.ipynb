{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:61: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:63: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:67: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:69: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:61: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:63: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:67: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "<>:69: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/n6/d3myd2vn4d1_p_t7tgm4dpsw0000gn/T/ipykernel_80562/21841146.py:61: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "  update_time, [paper_title](paper_url), paper_key, paper_url, repo_url, paper_abstract)\n",
      "/var/folders/n6/d3myd2vn4d1_p_t7tgm4dpsw0000gn/T/ipykernel_80562/21841146.py:63: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "  update_time, [paper_title](paper_url), paper_url, paper_url, repo_url, repo_url)\n",
      "/var/folders/n6/d3myd2vn4d1_p_t7tgm4dpsw0000gn/T/ipykernel_80562/21841146.py:67: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "  update_time, [paper_title](paper_url), paper_key, paper_url, paper_abstract)\n",
      "/var/folders/n6/d3myd2vn4d1_p_t7tgm4dpsw0000gn/T/ipykernel_80562/21841146.py:69: SyntaxWarning: 'list' object is not callable; perhaps you missed a comma?\n",
      "  update_time, [paper_title](paper_url), paper_url, paper_url, paper_abstract)\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import logging\n",
    "base_url = \"https://arxiv.paperswithcode.com/api/v0/papers/\"\n",
    "github_url = \"https://api.github.com/search/repositories\"\n",
    "arxiv_url = \"http://arxiv.org/\"\n",
    "\n",
    "def get_daily_papers(topic, query=\"agent\", max_results=2):\n",
    "    \"\"\"\n",
    "    @param topic: str\n",
    "    @param query: str\n",
    "    @return paper_with_code: dict\n",
    "    \"\"\"\n",
    "    # output\n",
    "    content = dict()\n",
    "    content_to_web = dict()\n",
    "    print(\"-----------------\")\n",
    "    print(f\"query is {query}\")\n",
    "    print(\"-----------------\")\n",
    "    search_engine = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    for result in search_engine.results():\n",
    "\n",
    "        paper_id = result.get_short_id()\n",
    "        paper_title = result.title\n",
    "        paper_url = result.entry_id\n",
    "        code_url = base_url + paper_id  # TODO\n",
    "\n",
    "        paper_abstract = result.summary.replace(\"\\n\", \" \")\n",
    "\n",
    "        paper_abstract = paper_abstract.replace(\"|\", \",\")\n",
    "        paper_abstract = paper_abstract.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "        primary_category = result.primary_category\n",
    "        publish_time = result.published.date()\n",
    "        update_time = result.updated.date()\n",
    "        comments = result.comment\n",
    "\n",
    "        # eg: 2108.09112v1 -> 2108.09112\n",
    "        ver_pos = paper_id.find('v')\n",
    "        if ver_pos == -1:\n",
    "            paper_key = paper_id\n",
    "        else:\n",
    "            paper_key = paper_id[0:ver_pos]\n",
    "        paper_url = arxiv_url + 'abs/' + paper_key\n",
    "\n",
    "        try:\n",
    "            # source code link\n",
    "            r = requests.get(code_url).json()\n",
    "            repo_url = None\n",
    "            if \"official\" in r and r[\"official\"]:\n",
    "                repo_url = r[\"official\"][\"url\"]\n",
    "\n",
    "            if repo_url is not None:\n",
    "                content[paper_key] = \"|**{}**|**{}**|[{}]({})|**[link]({})**|**{}**|\\n\".format(\n",
    "                    update_time, [paper_title](paper_url), paper_key, paper_url, repo_url, paper_abstract)\n",
    "                content_to_web[paper_key] = \"- {}, **{}**, Paper: [{}]({}), Code: **[{}]({})**\".format(\n",
    "                    update_time, [paper_title](paper_url), paper_url, paper_url, repo_url, repo_url)\n",
    "\n",
    "            else:\n",
    "                content[paper_key] = \"|**{}**|**{}**|[{}]({})|null|{}|\\n\".format(\n",
    "                    update_time, [paper_title](paper_url), paper_key, paper_url, paper_abstract)\n",
    "                content_to_web[paper_key] = \"- {}, **{}**, Paper: [{}]({}),{}\".format(\n",
    "                    update_time, [paper_title](paper_url), paper_url, paper_url, paper_abstract)\n",
    "\n",
    "            # TODO: select useful comments\n",
    "            comments = None\n",
    "            if comments != None:\n",
    "                content_to_web[paper_key] += f\", {comments}\\n\"\n",
    "            else:\n",
    "                content_to_web[paper_key] += f\"\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"exception: {e} with id: {paper_key}\")\n",
    "\n",
    "    data = {topic: content}\n",
    "    data_web = {topic: content_to_web}\n",
    "    return data, data_web\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n6/d3myd2vn4d1_p_t7tgm4dpsw0000gn/T/ipykernel_85439/3933386129.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search_engine.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14\n",
      "2024-11-14\n",
      "2024-11-14\n",
      "2024-11-14\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-12\n",
      "2024-11-13\n",
      "2024-11-13\n",
      "2024-11-12\n",
      "2024-11-11\n",
      "2024-11-11\n",
      "2024-11-11\n",
      "2024-11-11\n",
      "2024-11-11\n",
      "2024-11-10\n",
      "2024-11-14\n",
      "2024-11-09\n",
      "2024-11-09\n",
      "2024-11-09\n",
      "2024-11-09\n",
      "2024-11-08\n",
      "2024-11-08\n",
      "2024-11-08\n",
      "2024-11-08\n",
      "2024-11-08\n",
      "2024-11-08\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-07\n",
      "2024-11-06\n",
      "2024-11-06\n",
      "2024-11-06\n",
      "2024-11-06\n",
      "2024-11-05\n",
      "2024-11-05\n",
      "2024-11-05\n",
      "2024-11-06\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-04\n",
      "2024-11-03\n",
      "2024-11-01\n",
      "2024-11-01\n",
      "2024-11-01\n",
      "2024-11-01\n",
      "2024-11-01\n",
      "2024-11-04\n",
      "2024-10-31\n",
      "2024-10-31\n",
      "2024-10-31\n",
      "2024-10-31\n",
      "2024-10-30\n",
      "2024-10-30\n",
      "2024-10-30\n",
      "2024-10-31\n",
      "2024-10-30\n",
      "2024-10-30\n",
      "2024-10-30\n",
      "2024-10-30\n",
      "2024-10-29\n",
      "2024-10-29\n",
      "2024-10-28\n",
      "2024-10-28\n",
      "2024-10-28\n",
      "2024-10-28\n",
      "2024-10-28\n",
      "2024-10-28\n",
      "2024-11-11\n",
      "2024-10-28\n",
      "2024-11-05\n",
      "2024-10-27\n",
      "2024-10-27\n",
      "2024-10-26\n",
      "2024-10-26\n",
      "2024-10-25\n",
      "2024-10-30\n",
      "2024-10-24\n",
      "2024-10-24\n",
      "2024-10-25\n",
      "2024-10-24\n",
      "2024-10-24\n",
      "2024-11-01\n",
      "2024-10-23\n",
      "2024-10-23\n",
      "2024-10-23\n",
      "2024-10-23\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import logging\n",
    "base_url = \"https://arxiv.paperswithcode.com/api/v0/papers/\"\n",
    "github_url = \"https://api.github.com/search/repositories\"\n",
    "arxiv_url = \"http://arxiv.org/\"\n",
    "query = \"RAG\"\n",
    "max_results = 100\n",
    " \n",
    "\n",
    "search_engine = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=max_results,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "for result in search_engine.results():\n",
    "\n",
    "    paper_id = result.get_short_id()\n",
    "    paper_title = result.title\n",
    "    paper_comment = result.comment\n",
    "    publish_time = result.published.date()\n",
    "    update_time = result.updated.date()\n",
    "    # print(update_time)\n",
    "    print(str(update_time))\n",
    "    paper_url = result.entry_id\n",
    "    code_url = base_url + paper_id  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 'Accepted by ICANN 2024', found: 'ICANN 2024'\n",
      "In '8 pages, 4 figures, accepted by ECAI', found: 'ECAI'\n",
      "In '10 pages, 3 figures', no match found.\n",
      "In 'accepted by ACM MM 2024', found: 'ACM MM 2024'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 定义正则表达式\n",
    "pattern = r\"(?i)(?<=accepted by\\s).*\"\n",
    "\n",
    "# 定义一些示例文本\n",
    "texts = [\n",
    "    \"Accepted by ICANN 2024\",\n",
    "    \"8 pages, 4 figures, accepted by ECAI\",\n",
    "    \"10 pages, 3 figures\",\n",
    "    \"accepted by ACM MM 2024\"\n",
    "]\n",
    "\n",
    "# 遍历所有文本，使用正则表达式匹配\n",
    "for text in texts:\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        print(f\"In '{text}', found: '{match.group()}'\")\n",
    "    else:\n",
    "        print(f\"In '{text}', no match found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACM MM 2024\n"
     ]
    }
   ],
   "source": [
    "print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 'Accepted by  2024 AAAI', found conferences: AAAI\n",
      "In 'Submitted to NeurIPS', found conferences: NeurIPS\n",
      "In 'This is a CVPR2023 paper.', found conferences: CVPR 2023\n",
      "In 'Interesting results in ICLR', found conferences: ICLR\n",
      "In 'MICCAI 2022 had great papers', found conferences: MICCAI 2022\n",
      "In 'NAACL2021 was a virtual event', found conferences: NAACL 2021\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 定义正则表达式\n",
    "pattern = r\"(?i)\\b(AAAI|NeurIPS|ACL|CVPR|ICCV|ICML|IJCAI|EMNLP|COLING|NAACL|EACL|CoNLL|ICLR|INLG|ECCV|MICCAI|IROS|MIDL|ICIP|KDD|MIDC)(?:\\s?(\\d{4}))?\\b\"\n",
    "\n",
    "# 定义一些示例文本\n",
    "texts = [\n",
    "    \"Accepted by  2024 AAAI\",\n",
    "    \"Submitted to NeurIPS\",\n",
    "    \"This is a CVPR2023 paper.\",\n",
    "    \"Interesting results in ICLR\",\n",
    "    \"MICCAI 2022 had great papers\",\n",
    "    \"NAACL2021 was a virtual event\",\n",
    "\n",
    "]\n",
    "\n",
    "# 遍历所有文本，使用正则表达式匹配\n",
    "for text in texts:\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        # 格式化输出匹配结果\n",
    "        formatted_matches = [f\"{conf} {year}\".strip() for conf, year in matches]\n",
    "        print(f\"In '{text}', found conferences: {', '.join(formatted_matches)}\")\n",
    "    else:\n",
    "        print(f\"In '{text}', no conference name found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAACL 2021']\n",
      "NAACL 2021\n",
      "NAACL 2021\n"
     ]
    }
   ],
   "source": [
    "print(formatted_matches)\n",
    "print(', '.join(formatted_matches))\n",
    "a= ', '.join(formatted_matches)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17880\\1422836124.py:18: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search_engine.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defense Priorities in the Open-Source AI Debate: A Preliminary Assessment\n",
      "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate\n",
      "DebateQA: Evaluating Question Answering on Debatable Knowledge\n",
      "Interpreting and Mitigating Hallucination in MLLMs through Multi-agent Debate\n",
      "Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks\n",
      "MentalAgora: A Gateway to Advanced Personalized Care in Mental Health through Multi-Agent Debating and Attribute Control\n",
      "Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework with Debate-Driven Text Planning for Argument Generation\n",
      "Sequence Graph Network for Online Debate Analysis\n",
      "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\n",
      "Mining United Nations General Assembly Debates\n",
      "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction\n",
      "Improving Multi-Agent Debate with Sparse Communication Topology\n",
      "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs\n",
      "Evaluating the Performance of Large Language Models via Debates\n",
      "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework\n",
      "An Empirical Analysis on Large Language Models in Debate Evaluation\n",
      "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation\n",
      "Assisted Debate Builder with Large Language Models\n",
      "Reinforcement Learning from Multi-role Debates as Feedback for Bias Mitigation in LLMs\n",
      "Unraveling the Dynamics of Television Debates and Social Media Engagement: Insights from an Indian News Show\n",
      "Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Debated Topics\n",
      "Persuasion or Insulting? Unpacking Discursive Strategies of Gender Debate in Everyday Feminism in China\n",
      "A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning\n",
      "Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM\n",
      "Crisis talk: analysis of the public debate around the energy crisis and cost of living\n",
      "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements\n",
      "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate\n",
      "Debating with More Persuasive LLMs Leads to More Truthful Answers\n",
      "Systematic Biases in LLM Simulations of Debates\n",
      "Limits of Large Language Models in Debating Humans\n",
      "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate\n",
      "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models\n",
      "Combating Adversarial Attacks with Multi-Agent Debate\n",
      "A Philosophical Introduction to Language Models -- Part I: Continuity With Classic Debates\n",
      "Resolving the Debate Between Boltzmann and Gibbs Entropy: Relative Energy Window Eliminates Thermodynamic Inconsistencies and Allows Negative Absolute Temperatures\n",
      "Few-shot learning for automated content analysis: Efficient coding of arguments and claims in the debate on arms deliveries to Ukraine\n",
      "Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System\n",
      "FREDSum: A Dialogue Summarization Corpus for French Political Debates\n",
      "Playing Large Games with Oracles and AI Debate\n",
      "Use of explicit replies as coordination mechanisms in online student debate\n",
      "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs\n",
      "Scalable AI Safety via Doubly-Efficient Debate\n",
      "Research assessment under debate: disentangling the interest around the DORA declaration on Twitter\n",
      "Debate Helps Supervise Unreliable Experts\n",
      "Dynamics of toxic behavior in the Covid-19 vaccination debate\n",
      "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding\n",
      "Let Models Speak Ciphers: Multiagent Debate through Embeddings\n",
      "A Primer on Bayesian Neural Networks: Review and Debates\n",
      "Speaker attribution in German parliamentary debates with QLoRA-adapted large language models\n",
      "Political Context of the European Vaccine Debate on Twitter\n",
      "ChatGPT impacts in programming education: A recent literature overview that debates ChatGPT responses\n",
      "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
      "Understanding the Vegetable Oil Debate and Its Implications for Sustainability through Social Media\n",
      "Debating the Reliability and Robustness of the Learned Hamiltonian in the Traversable Wormhole Experiment\n",
      "The debate over QKD: A rebuttal to the NSA's objections\n",
      "Recoil momentum of an atom absorbing light in a gaseous medium and the Abraham-Minkowski debate\n",
      "DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs\n",
      "Beyond Active Engagement: The Significance of Lurkers in a Polarized Twitter Debate\n",
      "Methodological Reflections on the MOND/Dark Matter Debate\n",
      "An Interleaving Semantics of the Timed Concurrent Language for Argumentation to Model Debates and Dialogue Games\n",
      "Impact of the Covid 19 outbreaks on the italian twitter vaccination debat: a network based analysis\n",
      "God and the Big-Bang: Past and Modern Debates Between Science and Theology\n",
      "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data\n",
      "Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate\n",
      "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate\n",
      "Political corpus creation through automatic speech recognition on EU debates\n",
      "Digitization of the Australian Parliamentary Debates, 1998-2022\n",
      "The Full Rights Dilemma for A.I. Systems of Debatable Personhood\n",
      "ASR Bundestag: A Large-Scale political debate dataset in German\n",
      "Biased processing and opinion polarization: experimental refinement of argument communication theory in the context of the energy debate\n",
      "Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E\n",
      "Global misinformation spillovers in the online vaccination debate before and during COVID-19\n",
      "Un discours et un public \"Gilets Jaunes\" au coeur du Grand Débat National? Combinaison des approches IA et textométriques pour l'analyse de discours des plateformes \"Grand Débat National\" et \"Vrai débat\"\n",
      "Two-Turn Debate Doesn't Help Humans Answer Hard Reading Comprehension Questions\n",
      "Explaining Image Classification with Visual Debates\n",
      "The Debate Over Understanding in AI's Large Language Models\n",
      "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates\n",
      "Tackling problems, harvesting benefits -- A systematic review of the regulatory debate around AI\n",
      "Domain-Independent Deception: Definition, Taxonomy and the Linguistic Cues Debate\n",
      "Moral Narratives Around the Vaccination Debate on Facebook\n",
      "Oxford-style Debates in Telecommunication and Computer Science Education\n",
      "The ParlaSent-BCS dataset of sentiment-annotated parliamentary debates from Bosnia-Herzegovina, Croatia, and Serbia\n",
      "Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates\n",
      "A Holistic Framework for Analyzing the COVID-19 Vaccine Debate\n",
      "Modeling Political Activism around Gun Debate via Social Media\n",
      "Echoes through Time: Evolution of the Italian COVID-19 Vaccination Debate\n",
      "Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions\n",
      "Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks\n",
      "The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments\n",
      "GraphNLI: A Graph-based Natural Language Inference Model for Polarity Prediction in Online Debates\n",
      "Ensemble of Opinion Dynamics Models to Understand the Role of the Undecided in the Vaccination Debate\n",
      "The co-evolutionary relationship between digitalization and organizational agility: Ongoing debates, theoretical developments and future research perspectives\n",
      "DEBACER: a method for slicing moderated debates\n",
      "The Packet Number Space Debate in Multipath QUIC\n",
      "A glimpse into Feynman's contributions to the debate on the foundations of quantum mechanics\n",
      "Project Debater APIs: Decomposing the AI Grand Challenge\n",
      "The brain is a computer is a brain: neuroscience's internal debate and the social significance of the Computational Metaphor\n",
      "The Impact of Disinformation on a Controversial Debate on Social Media\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import logging\n",
    "base_url = \"https://arxiv.paperswithcode.com/api/v0/papers/\"\n",
    "github_url = \"https://api.github.com/search/repositories\"\n",
    "arxiv_url = \"http://arxiv.org/\"\n",
    "query = \"debate\"\n",
    "max_results = 100\n",
    " \n",
    "\n",
    "search_engine = arxiv.Search(\n",
    "    query=f'ti:{query}',\n",
    "    max_results=max_results,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    \n",
    ")\n",
    "\n",
    "for result in search_engine.results():\n",
    "\n",
    "    paper_id = result.get_short_id()\n",
    "    paper_title = result.title\n",
    "    paper_comment = result.comment\n",
    "    print(paper_title)\n",
    "    paper_url = result.entry_id\n",
    "    code_url = base_url + paper_id  # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feishu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
